![Python](https://img.shields.io/badge/Python-3.10+-blue)
![FastAPI](https://img.shields.io/badge/FastAPI-Backend-green)
![FAISS](https://img.shields.io/badge/FAISS-VectorDB-orange)
![LLM](https://img.shields.io/badge/LLM-LLaMA--3-purple)
![License](https://img.shields.io/badge/License-MIT-lightgrey)

# ğŸš€ Production-Grade RAG AI Assistant

### Retrieval-Augmented Generation (RAG) API using FastAPI, FAISS, SentenceTransformers & Groq LLaMA-3

------------------------------------------------------------------------

## ğŸ“Œ Overview

This project implements a production-style Retrieval-Augmented
Generation (RAG) system that delivers accurate, document-grounded
responses with low latency.

Instead of relying solely on an LLM's internal knowledge (which can
cause hallucinations), this system:

1.  Retrieves semantically relevant document chunks
2.  Injects them into a structured prompt
3.  Generates grounded responses using Groq's LLaMA-3

The architecture is modular, scalable, and LLM-agnostic --- making it
suitable for enterprise knowledge bases, AI copilots, and intelligent
search systems.

------------------------------------------------------------------------

## ğŸ§  System Architecture

    PDF Documents
          â†“
    Text Chunking (with overlap)
          â†“
    SentenceTransformers Embeddings
          â†“
    FAISS Vector Index
          â†“
    User Query â†’ Query Embedding
          â†“
    Top-K Similarity Search
          â†“
    Context Injection
          â†“
    Groq LLaMA-3
          â†“
    Grounded Response

------------------------------------------------------------------------

## âœ¨ Key Features

-   PDF document ingestion & preprocessing pipeline\
-   Dense semantic embeddings via SentenceTransformers\
-   FAISS-based similarity search (low-latency retrieval)\
-   Prompt grounding to reduce hallucinations\
-   FastAPI REST API with automatic Swagger docs\
-   Modular, LLM-agnostic architecture\
-   Secure environment-based configuration

------------------------------------------------------------------------

## ğŸ›  Tech Stack

-   Python\
-   FastAPI\
-   FAISS\
-   SentenceTransformers\
-   Groq (LLaMA-3)\
-   Uvicorn\
-   Pydantic

------------------------------------------------------------------------

## ğŸ“‚ Project Structure

    rag-ai-assistant/
    â”‚
    â”œâ”€â”€ app/
    â”‚   â”œâ”€â”€ main.py
    â”‚   â”œâ”€â”€ ingestion.py
    â”‚   â”œâ”€â”€ retrieval.py
    â”‚   â”œâ”€â”€ llm.py
    â”‚   â””â”€â”€ config.py
    â”‚
    â”œâ”€â”€ data/
    â”‚   â””â”€â”€ sample.pdf
    â”‚
    â”œâ”€â”€ requirements.txt
    â”œâ”€â”€ .env.example
    â”œâ”€â”€ .gitignore
    â””â”€â”€ README.md

------------------------------------------------------------------------

## âš™ï¸ Installation & Setup

### Clone Repository

    git clone https://github.com/yourusername/rag-ai-assistant.git
    cd rag-ai-assistant

### Create Virtual Environment

    python -m venv venv

Activate:

Mac/Linux:

    source venv/bin/activate

Windows:

    venv\Scripts\activate

### Install Dependencies

    pip install -r requirements.txt

### Configure Environment Variables

Create a `.env` file:

    GROQ_API_KEY=your_api_key_here

### Run Server

    uvicorn app.main:app --reload

Swagger Docs: http://127.0.0.1:8000/docs

------------------------------------------------------------------------

## ğŸ“¡ API Endpoints

### Ingest Documents

POST `/ingest`

### Query System

POST `/query`

Example Request:

    {
      "question": "What is Retrieval-Augmented Generation?"
    }

Example Response:

    {
      "answer": "Retrieval-Augmented Generation retrieves relevant document chunks and injects them into the prompt before generating a response..."
    }

------------------------------------------------------------------------

## ğŸ¯ Why RAG?

-   Prevents hallucinations\
-   Grounds LLM output in real data\
-   Scales to large document collections\
-   Avoids costly fine-tuning

------------------------------------------------------------------------

## ğŸš€ Future Improvements

-   Streaming responses\
-   Hybrid retrieval (BM25 + Dense Search)\
-   Authentication & rate limiting\
-   Docker containerization\
-   Cloud deployment\
-   Evaluation metrics (Recall@K, MRR)\
-   Redis caching

------------------------------------------------------------------------

## ğŸ One-Line Interview Summary

Built a production-ready RAG system using FastAPI and FAISS that
performs semantic search over custom documents and generates grounded
responses using Groq's LLaMA-3.
